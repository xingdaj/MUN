# affine_Coupling — Variational Inference with Affine Normalizing Flows and SEM

> This README explains the **theory and design** behind `affine_Coupling.py`: how it couples a **normalizing flow** (ActNorm + Affine Coupling) with a **Spectral Element Method (SEM)** forward simulator and an **adjoint** to perform **Bayesian inversion** of a 12-D B-spline boundary.

---

## 1) Big picture

We want the posterior over a 12-dimensional latent vector  
\[
\mathbf z\in\mathbb R^{12}
\]
that encodes **offsets to 6 B-spline control points** (x/z per node). These offsets define a **closed cubic B-spline interface** separating two constant velocities \(v_{\text{in}}, v_{\text{out}}\). Given a source and a circular array of receivers, the **SEM forward model** generates synthetic seismograms. With observed data \( \mathbf y \), we define a (Gaussian) likelihood and a simple Gaussian prior on \( \mathbf z \), then train a **normalizing flow** \(q_\phi(\mathbf z)\) by maximizing the **ELBO**.

---

## 2) Geometry: B-spline boundary parameterization

- The boundary is a **closed cubic B-spline** with **6 control points** (periodicity via duplicating the first control point \(k=3\) times).  
- The **latent** \(\mathbf z\) holds **Gaussian offsets** applied to a fixed template of control points \(\mathbf C_{\text{base}} \in \mathbb R^{6\times 2}\):  
  \[
  \mathbf C(\mathbf z)=\mathbf C_{\text{base}}+\operatorname{reshape}(\mathbf z,6,2).
  \]
- A signed-distance field to the spline is computed, and a **soft interface** blends \((v_{\text{in}}, v_{\text{out}})\) using a sigmoid with width \(\tau\). This yields a **nodal velocity** field \(v(\mathbf x\,;\mathbf z)\) on the SEM grid.

---

## 3) Forward model: SEM wave propagation

- The SEM mesh is structured GLL (order \(p\)), with **PML** on all sides.  
- Time stepping uses a central-difference scheme with CFL-safe \(dt\) (auto-adjusted if needed).  
- Receivers are placed on a circle of given radius; source is a Ricker wavelet.  
- Given \(\mathbf z\), the SEM produces synthetic traces \(\mathbf y_{\text{syn}}(\mathbf z)\).  
- The script builds a **single simulator instance** from a configuration dict and reuses it via a small **cache** for efficiency.

---

## 4) Data & likelihood

Observed data \(\mathbf y\) are generated by running SEM once at a “true” \(\mathbf z_{\star}\) and then **adding Gaussian noise** with standard deviation \(\sigma\):
\[
p(\mathbf y\mid \mathbf z)
=\mathcal N\bigl(\mathbf y;\ \mathbf y_{\text{syn}}(\mathbf z),\ \sigma^2\mathbf I\bigr).
\]
The **log-likelihood** is (up to a constant)
\[
\log p(\mathbf y\mid \mathbf z)
=-\tfrac1{2\sigma^2}\|\mathbf y-\mathbf y_{\text{syn}}(\mathbf z)\|_2^2.
\]
The script stores both clean and noisy observations and fixes \(\sigma\) (configurable).

---

## 5) Prior on geometry

The prior over \(\mathbf z\) is **factorized Gaussian**, centered at **0** (i.e., at the base control points) with a chosen standard deviation \(t_{\text{std}}\). This encodes a belief that each control-point offset is roughly Gaussian around the template.

---

## 6) Normalizing flow: ActNorm + Affine Coupling

We construct \(q_\phi(\mathbf z)\) by pushing a simple base Gaussian through a sequence of **invertible transforms**.

### 6.1 Base
\[
\mathbf x\sim\mathcal N(\boldsymbol\mu_0,\boldsymbol\Sigma_0),\qquad \mathbf z=f_\phi(\mathbf x).
\]

### 6.2 Layers
Each flow block is:
1. **ActNorm** (per-dimension affine with data-dependent init)  
2. **Affine Coupling** (mask-based split; a small MLP predicts scale/shift for the “active” subset)

Let \(\mathbf u\) be the identity (masked-frozen) part and \(\mathbf v\) the transformed part. The coupling computes
\[
\mathbf v'=\mathbf v\odot\exp s(\mathbf u)+t(\mathbf u),
\quad
\log\Bigl|\det\frac{\partial \mathbf v'}{\partial \mathbf v}\Bigr|=\sum s(\mathbf u).
\]
With alternating masks, the composition remains **bijective** and **cheap-Jacobian** (log-det is just a sum). The full log-density follows the **change of variables**:
\[
\log q_\phi(\mathbf z)
=\log \mathcal N(\mathbf x\,;\boldsymbol\mu_0,\boldsymbol\Sigma_0)
+\sum_{\ell}\log\left|\det\frac{\partial f_\ell}{\partial\text{input}}\right|.
\]

---

## 7) Objective: ELBO and Monte-Carlo estimate

We maximize
\[
\mathcal L(\phi)
=\mathbb E_{q_\phi(\mathbf z)}\big[\log p(\mathbf y\mid \mathbf z)\big]
+\mathbb E_{q_\phi(\mathbf z)}\big[\log p(\mathbf z)-\log q_\phi(\mathbf z)\big].
\]

### 7.1 Estimation
At each epoch, draw \(n\) samples \(\{\mathbf z^{(i)}\}_{i=1}^n\sim q_\phi\) and compute
\[
\widehat{\mathcal L}
=\frac1n\sum_{i=1}^n\Big(\log p(\mathbf y\mid \mathbf z^{(i)})
+\log p(\mathbf z^{(i)})-\log q_\phi(\mathbf z^{(i)})\Big).
\]
The script uses a **linear schedule** to **increase** the number of ELBO samples across epochs (variance reduction as optimization progresses).

---

## 8) Injecting SEM gradients with a custom autograd

Key challenge: \(\log p(\mathbf y\mid \mathbf z)\) needs **\(\partial/\partial\mathbf z\)** through the SEM. The script defines a **custom `torch.autograd.Function`** that:

1. **Forward**: calls `SEMSimulation.run_forward_and_adjoint(...)` with current B-spline control points to get both **log-likelihood** and **its gradient w.r.t. the 12 offsets** (from the SEM adjoint).  
2. **Backward**: multiplies the upstream scalar gradient and **injects** the adjoint gradient into the graph, so it **back-propagates** through the flow parameters \(\phi\).

This provides **exact first-order information** from the PDE constraint without differentiating through the entire time-stepping unrolled graph. It’s the standard *adjoint-state* strategy:  
\[
\frac{\partial \log p(\mathbf y\mid \mathbf z)}{\partial \mathbf z}
=\underbrace{\frac{\partial \log p}{\partial \mathbf u}}_{\text{data misfit}}
\cdot
\underbrace{\frac{\partial \mathbf u}{\partial \mathbf v}}_{\text{wave eq.}}
\cdot
\underbrace{\frac{\partial \mathbf v}{\partial \mathbf z}}_{\text{velocity via B-spline}}.
\]
Only the final vector is needed at training time, which the SEM adjoint supplies efficiently.

---

## 9) Optimization hygiene

- **Double precision** (`torch.float64`) for numerical stability.  
- **Gradient monitoring** (norms, min/max, explosions); **clipping/scaling** when norms exceed a threshold.  
- **ActNorm** data-dependent initialization to avoid early drift.  
- **Small coupling scale** via `tanh` to tame Jacobian determinants early on.  
- **Simulator cache** to avoid redundant setup work across ELBO calls.  
- **Diagnostics**: ELBO curves, gradient history, posterior boundary plots, and 12-D marginal histograms.

---

## 10) What the training loop does (conceptual pseudocode)

```python
for epoch in range(num_epochs):
    # 1) sample z~q_phi
    z_samples, logdet = flow.forward(n_samples = schedule(epoch))
    log_q = flow.log_prob(z_samples)               # change-of-variables
    log_pz = Normal(0, tstd).log_prob(z_samples).sum(-1)

    # 2) for each z_i, call SEM adjoint to get log p(y|z_i) and d/dz_i log p(y|z_i)
    log_py = [ sem_loglik_with_adjoint(z_i, y, sigma, dt, nt) for z_i in z_samples ]

    # 3) Monte-Carlo ELBO
    elbo = mean( log_py + log_pz - log_q )

    # 4) backprop (adjoint gradients enter here), clip/scale, and step
    (-elbo).backward()
    maybe_clip_gradients()
    optimizer.step()
```

All three expectation terms are handled consistently, and the **likelihood gradient** is supplied by the **adjoint** through the custom autograd hook.

---

## 11) Outputs

- **Posterior samples** (`posterior_samples.npy`) and **trained flow** weights (`trained_flow_model.pth`).  
- **Figures**:
  - `posterior_boundary_epoch_XXXX.png` (evolution),
  - `nf_posterior_distributions.png` (12 histograms with prior/true/posterior mean),
  - `nf_boundary_comparison.png` (true & prior vs posterior mean + sample cloud),
  - `elbo_history_real.png` and `gradient_history_real.png`.

---

## 12) Practical guidance & pitfalls

- **Likelihood scale**: Ensure `noise_std` reflects the actual noise; too small \(\sigma\) makes the likelihood overly sharp and can destabilize training.  
- **CFL vs dt**: The SEM will adjust `dt`—keep an eye on logs; if reduced a lot, consider smaller polynomial order or coarser mesh to retain speed.  
- **Flow capacity**: Start with fewer blocks/hidden units and a small coupling `scale_factor`, then scale up once ELBO is stable.  
- **Initialization**: Because the base prior is centered at zero offsets, the initial posterior should **resemble the prior**; dramatic explosions early often point to ActNorm not initialized, step size too large, or too aggressive scales.  
- **Identifiability**: With circular receivers, multiple shapes may explain data similarly; posterior broadness isn’t always a bug—check diagnostics.

---

## 13) Extending the method

- **Richer likelihoods**: Time-windowed misfit, multi-component sensors, frequency-domain terms.  
- **Alternative flows**: Rational-Quadratic Splines (RQS), autoregressive layers (MAF/IAF) if you need sharper posteriors; keep ActNorm and cautious init.  
- **Hierarchical priors**: Learn \(\sigma\) or \(t_{\text{std}}\) with hyperpriors (but guard against degeneracy).  
- **Physics variations**: Anisotropy, attenuation, multi-source experiments—only the adjoint API must return \(\log p\) and \(\partial\log p/\partial\mathbf z\).

---

## 14) Repro checklist

-  Set SEM config (domain/time/method/PML/source/receivers/velocity).  
-  Generate observations once (clean & noisy).  
-  Verify ELBO increases and gradients remain finite (monitor plots).  
-  Inspect posterior figures for **contraction toward the true boundary**.  
-  Save posterior and trained flow; re-run forward at posterior mean for sanity check.
